{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pyodbc\n",
    "from market_growth_analysis.etl.stagging import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sys.path.append('../conf') \n",
    "# Load the YAML file\n",
    "with open('../../conf/global.yml', 'r') as f:\n",
    "    columns = yaml.safe_load(f)\n",
    "\n",
    "# Load the YAML file\n",
    "with open('../../conf/local.yml', 'r') as f:\n",
    "    credentials = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "# Get credentials\n",
    "driver = credentials['warehouse_db']['driver']\n",
    "server = credentials['warehouse_db']['server']\n",
    "database = credentials['warehouse_db']['database']\n",
    "trusted = credentials['warehouse_db']['trusted_connection']\n",
    "user = credentials['warehouse_db']['user']\n",
    "password = credentials['warehouse_db']['password']\n",
    "\n",
    "# Create connection db\n",
    "conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      f'Server={server};'\n",
    "                      f'Database={database};'\n",
    "                      'Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\germa\\AppData\\Local\\Temp/ipykernel_13112/764186177.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql(join_tables_query, conn)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "join_tables_query = '''\n",
    "select *\n",
    "from DIM_COMPANY dim\n",
    "join FACT_BALANCE_SHEET bs on bs.ticker = dim.ticker\n",
    "join FACT_CASH_FLOW_STATEMENT cfs on cfs.PK = bs.PK\n",
    "join FACT_INCOME_STATEMENT fis on fis.PK = bs.PK\n",
    "join FACT_PRICES fp on fp.PK = bs.PK\n",
    "join FACT_RATIOS fr on fr.PK = bs.PK\n",
    "'''\n",
    "data = pd.read_sql(join_tables_query, conn)\n",
    "# drop duplicated columns\n",
    "data = data.T.drop_duplicates().T\n",
    "data['Date'] = pd.to_datetime(data['Date'], format=\"%Y/%m/%d\").dt.strftime(\"%Y/%m/%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticker', 'company_full_name', 'country', 'industry', 'sector', 'PK',\n",
       "       'Date', 'Cash On Hand', 'Receivables', 'Inventory', 'Pre-Paid Expenses',\n",
       "       'Other Current Assets', 'Total Current Assets',\n",
       "       'Property, Plant, And Equipment', 'Long-Term Investments',\n",
       "       'Goodwill And Intangible Assets', 'Other Long-Term Assets',\n",
       "       'Total Long-Term Assets', 'Total Assets', 'Total Current Liabilities',\n",
       "       'Long Term Debt', 'Other Non-Current Liabilities',\n",
       "       'Total Long Term Liabilities', 'Total Liabilities', 'Common Stock Net',\n",
       "       'Retained Earnings (Accumulated Deficit)', 'Comprehensive Income',\n",
       "       'Other Share Holders Equity', 'Share Holder Equity',\n",
       "       'Total Liabilities And Share Holders Equity', 'Net Income/Loss',\n",
       "       'Total Depreciation And Amortization - Cash Flow',\n",
       "       'Other Non-Cash Items', 'Total Non-Cash Items',\n",
       "       'Change In Accounts Receivable', 'Change In Inventories',\n",
       "       'Change In Accounts Payable', 'Change In Assets/Liabilities',\n",
       "       'Total Change In Assets/Liabilities',\n",
       "       'Cash Flow From Operating Activities',\n",
       "       'Net Change In Property, Plant, And Equipment',\n",
       "       'Net Change In Intangible Assets', 'Net Acquisitions/Divestitures',\n",
       "       'Net Change In Short-term Investments',\n",
       "       'Net Change In Long-Term Investments',\n",
       "       'Net Change In Investments - Total', 'Investing Activities - Other',\n",
       "       'Cash Flow From Investing Activities', 'Net Long-Term Debt',\n",
       "       'Net Current Debt', 'Debt Issuance/Retirement Net - Total',\n",
       "       'Net Common Equity Issued/Repurchased',\n",
       "       'Net Total Equity Issued/Repurchased',\n",
       "       'Total Common And Preferred Stock Dividends Paid',\n",
       "       'Financial Activities - Other', 'Cash Flow From Financial Activities',\n",
       "       'Net Cash Flow', 'Stock-Based Compensation',\n",
       "       'Common Stock Dividends Paid', 'Revenue', 'Cost Of Goods Sold',\n",
       "       'Gross Profit', 'Research And Development Expenses', 'SG&A Expenses',\n",
       "       'Other Operating Income Or Expenses', 'Operating Expenses',\n",
       "       'Operating Income', 'Total Non-Operating Income/Expense',\n",
       "       'Pre-Tax Income', 'Income Taxes', 'Income After Taxes', 'Other Income',\n",
       "       'Income From Continuous Operations',\n",
       "       'Income From Discontinued Operations', 'Net Income', 'EBITDA', 'EBIT',\n",
       "       'Basic Shares Outstanding', 'Shares Outstanding', 'Basic EPS',\n",
       "       'EPS - Earnings Per Share', 'longevity', 'Close', 'Volume', 'Growth -1',\n",
       "       'Growth +1', 'Growth +5', 'Growth -10', 'Growth -5',\n",
       "       'Enterprise Value (EV)', 'EV / Revenue', 'EV / EBITDA', 'EV / EBIT',\n",
       "       'EV / Invested Capital', 'Free Cash Flow (FCF)', 'EV / Free Cash Flow',\n",
       "       'P/E', 'P/S', 'P/CF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States           39989\n",
       "Canada                   1557\n",
       "China                    1229\n",
       "Israel                    753\n",
       "United Kingdom            709\n",
       "Bermuda                   443\n",
       "Ireland                   354\n",
       "Brazil                    350\n",
       "Netherlands               194\n",
       "Hong Kong, SAR China      193\n",
       "Switzerland               189\n",
       "Greece                    189\n",
       "Argentina                 178\n",
       "Germany                   150\n",
       "South Korea               142\n",
       "Mexico                    142\n",
       "Japan                     129\n",
       "Singapore                 127\n",
       "India                     123\n",
       "Australia                 119\n",
       "Luxembourg                117\n",
       "Taiwan                    117\n",
       "France                    115\n",
       "South Africa               94\n",
       "Cayman Islands             94\n",
       "Chile                      86\n",
       "Colombia                   60\n",
       "Spain                      56\n",
       "Belgium                    54\n",
       "Peru                       53\n",
       "Sweden                     53\n",
       "Denmark                    45\n",
       "Italy                      42\n",
       "Panama                     28\n",
       "Norway                     16\n",
       "Turkey                     16\n",
       "Philippines                14\n",
       "Indonesia                  14\n",
       "Finland                    14\n",
       "Jersey                     10\n",
       "Bahamas                     6\n",
       "Russia                      4\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Medical - Biomedical and Genetics        3573\n",
       "Medical - Drug Manufacturing             1682\n",
       "Technology Services                      1232\n",
       "Banks - Northeast                        1199\n",
       "REIT - Other Equity Trusts               1165\n",
       "                                         ... \n",
       "Nanotechnology Equipment and Services      11\n",
       "Electronics - Military Systems             11\n",
       "Retail - Mail Order & Direct                9\n",
       "Printing & Trade Machinery                  8\n",
       "Periodical Publishing                       2\n",
       "Name: industry, Length: 252, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['industry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_study = columns['columns_prices'] + columns['columns_ratios'] + columns['column_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = 'Medical - Biomedical and Genetics'\n",
    "\n",
    "data_filtered_industry = data[(data['industry'] == industry)&(data['country'] == 'United States')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_filtered_industry['ticker'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_study = data_filtered_industry[columns_to_study]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_study = data_to_study.dropna(subset='Growth +1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_study_filled = data_to_study.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longevity', 'Close', 'Volume', 'Growth -1', 'Growth +1', 'Growth +5',\n",
       "       'Growth -10', 'Growth -5', 'Enterprise Value (EV)', 'EV / Revenue',\n",
       "       'EV / EBITDA', 'EV / EBIT', 'EV / Invested Capital',\n",
       "       'Free Cash Flow (FCF)', 'EV / Free Cash Flow', 'P/E', 'P/S', 'P/CF',\n",
       "       'PK', 'ticker', 'Date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_study_filled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Growth +5', \n",
    "                #    'Date', \n",
    "                #    'ticker', \n",
    "                   'Close', \n",
    "                   'Volume', \n",
    "                   'longevity', \n",
    "                   'Enterprise Value (EV)', \n",
    "                   'Free Cash Flow (FCF)', \n",
    "                   'EV / EBITDA', \n",
    "                   'Growth -5', \n",
    "                   'Growth -1', \n",
    "                   'Growth -10',\n",
    "                   'EV / Free Cash Flow',\n",
    "                   'EV / Invested Capital'\n",
    "                   ]\n",
    "data_to_study_filled_drop = data_to_study_filled.drop(columns=columns_to_drop)\n",
    "data_to_study_filled_drop = data_to_study_filled_drop.set_index(keys='PK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PK\n",
       "4054    -54.476668\n",
       "4056    -70.671142\n",
       "5876    -60.640301\n",
       "5812     -1.392397\n",
       "5813    -17.480541\n",
       "           ...    \n",
       "47981   -23.132186\n",
       "47984   -56.637555\n",
       "47983   -64.385692\n",
       "32506    33.555561\n",
       "44047    57.598050\n",
       "Name: Growth +1, Length: 2387, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_study_filled_drop['Growth +1']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2387.000000\n",
       "mean       10.454595\n",
       "std       204.415933\n",
       "min       -99.444444\n",
       "25%       -57.321640\n",
       "50%       -20.000000\n",
       "75%        27.615242\n",
       "max      5346.428741\n",
       "Name: Growth +1, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_to_study_filled_drop['Growth +1']*100).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEICAYAAABBBrPDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWW0lEQVR4nO3deZhldX3n8fcnLBIFWexWNgGNxIhxidagjBuKKJgoqDiBceKG05MoScb45FFHR9R53MYkjnGdVhEwiiYmmo7ivnSDClr4oCOiY9sj0izS0IBsLsh3/jin8FLc+lV103eh6/16nnrqnv176t57Puf3O+feSlUhSdJCfmvSBUiSpptBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCt0pSSe43hu0kyQeSXJ3kG6Pe3h2V5DVJ/mEK6nhykk9s5bLPTvK5xvTDk2zc6uKmUJLnJTl7YPj6JPcdwXa+keSB23q908SgmEJJfpzkpv6FfXWSTyW596TrmjP/DbgVHg0cCexfVYcusI19krw3yaX932FDklOT/N4d2O6ixnnAnPc8X986kPdeD7ypX/Z/J3n3wLp2SnLDAuMeWVUfqqonDUwby0lBv62vJHnhIvPcJckbk/yk/5v8MMlfJ8kSt3FQv087LjRPVe1aVRu2tP4l+BvgdSNY79QwKKbXU6tqV2Af4KfA2ydcz7Z0IPDjqrph2MQk9wC+BtwVeAywG/AwYC1dwAxbZsEDxCT14fa8xixP7Q9guw4eyIes598Bu1fVOf2odcBjB2aZAX5C9/caHAdw3pZXPnb/BBwBPIXu+f4TYBXwtkkWBUt6ba0BHp9k73HUMwkGxZSrqp8DHwMOmRuXZPckpyfZlOSiJK9K8ltJ9kqyMclT+/l2TbI+yXP64VOTvCfJ55Ncl2RtkgOHbbexjQcA7wEO68+Cr1lg+X2TrEmyua/hP/fjTwTeN7D8a4cs/hLgZ8CfVNWPqnNNVX2gqt7er2fuDPLEJD8BvtTX96q+3iv6+nfv5z8tyUv7x/v1y764H/6dvs67AZ8G9h04y9+3r2nnfn3XJbkgycz8okfsaLqgnLMOeECSFf3wY4CPAHebN+7rVfWrwVZgknX99G/3+/jHcytN8tL+b3dZkucPjB/6euin3aZrbvDsPsnr+zre0W/rHfN3LMkRwJOAZ1bVd6vq5j4Q/xPw4rmWT7oW2BMHlhvc7tw+XdNv57Ah26mBdd0lyd/0LZif9u+L3+6nHd6/j16W5HLgA0lWJPlkkmv618pZc/vfv0fPA558+6dt+2BQTLkkdwX+GDhnYPTbgd2B+wKPA54DPL+qNgMvAN6b5J7AW4Hzq+r0gWWfDfwPYAVwPvChBTa90DYuBP6U7gC0a1XtscDyHwE2AvsCxwFvSPKEqnr/vOVPHrLsE4GPV9UtC6x70OOAB9C9SZ/X/zy+r3tXYO7AtBY4fGCZDfzmjPxxwFl9C+do4NKBs/xL+3me1u/THnRnkLc74G2lD/UH388leUhjvgcBP5gbqKqLgYv4TQviscBZdC2xwXHrmKeq5vb7If0+frQf3pvuOd8POBF4Z5I9+2lDXw+L7VxVvbKv66R+WycNme1I4Nx+nwaXPZfuNXTEYtvhN8/lHv12vr7I/G8Cfhd4KHA/un1+9cD0vYG96Fq/q4CX9rWsBO4F/Ddg8PuPLgRaz9+dmkExvT7Rn61fS/dGegtAkh2A44FXVNV1VfVj4G/pmupU1efomvFfpGvG/5d56/1UVa2rql8Ar6Q7s7/N9Y/FtrGYfn2PAl5WVT+vqvPpWhHPWeK+rwAuH1jf0/ozuety+37811TVDVV1E10I/l1Vbaiq64FXAMen6zpYCzy6Pwt8LPA/+xqhO/Ctpe3sqjqzqn4NfJBtc1B4NnAQ3cHoy8Bnk+yxwLx7ANfNG7cWeGy/T4fSnUycNTDuUSy+X4N+Bbyuqn5VVWcC1wP3v6OvhyVYAVy2wLTL+unbTJLQHfxfUlWbq+o64A10+zjnFuDkqvpF/9r6FV038IH93+esuu0X5V1H9xxtlwyK6XVsf7a+C3ASsDZdH+gKYCe6s8k5F9GdEc1ZDfw+cGpVXTVvvbeetfUH0810Z/2DlrKNln2BuTfg1ix/Fd2bcq7ONf3f4iXAzvPmHTwL3XdIzTsC96qqHwE30J1BPgb4JHBpkvuztKC4fODxjcAuWaDvOsl3+mC7BviPwLvmhpO8a2C/vlpVN1XVjVX1RuAabnuNYdDVdH33g+auUzwI2FBVNwJnD4z7beDcRfZr0FVVdfO8/dyVO/56WMyVDDzf8+zTT9+WVtJd/zpv4Hn6TD9+zqa+S2nOW4D1wOfS3Vjx8nnr3I3u+dsuGRRTrqp+XVX/Avya7m6hK+nObgavLRwAXAK3tgZWA6cDL8rt72y5tfWQZFe65vWl8+ZpboPbNrmHuRTYK8nggW1w+cV8ETh2rg94EYO1XMrta76Z7mYA6MLgOGDnqrqkH34usCddN9z89W2VqnpwVe3Rh9uHgRfNDVfVixbZl4Xu8vkOXVfJoHV0LZs/pGtJAFxA9xz/IfDNeQe7rbXY6+EGugPvnPkXdRf7m34BeMSQlu0j6PblS0vYzpY8b1cCNwEPHHhedu9vHhm6vr4l9dKqui9dN+Rf9ddW5jwA+PYW1HCnYlBMuXSOoTuYXdh3ffwj8Poku6W7GP1XwNxFvbm+0xfQnQWd3ofHnKckeXSSnemuVZwzpG94sW38FNi/X8ft9Ov7GvDGJLskeTBdn/dSP4vwd/3+frC/0Jw+dB66yHJnAC9Jcp8+BN8AfHTgLHktXetsrt/+K/3w2f0+z+3bPdJfBB+VJAckeVSSnfu/0V/Tnbl/dYFFzqRr+dyqqtb39f4lfVD03SHn9uNud31iwE/prjcsagmvh/PpursO6P9ur9iSbVXVF+hODv45yQOT7JDkkf36311VPxzYzvHpbvudoQv9OZvouosW3af+2td7gbf21/LmbnBY8GJ0kj9Kcr++2+pauhO3W/ppuwAPBz6/2LbvrAyK6fVvSa6nu/vn9cBzq+qCftqf051dbaDravgwcEqSh9O9gZ/Tv7nfTBcag83kDwMn03U5PZzuzpJhhm6jn/YlujPXy5Ms1C1wAl3/+6XAx+n6e7+wlB2vqiuBRwI/77d9Hd1BYjfgzxqLnkJ3/WAd8P/65f98YPrafh1zB9Cz6c5Qbz2gVtX36QJnQ98tMb9bblvZDXg3XZfSJcBRwNFDugrn6voWcG1/lj1oHV2XyWDAnAXck3ZQvAY4rd/H/7CEehd8PVTV54GP0rV6zqPr1hv0NuC4dJ8J+vsF1v9Muus0n6G7NvIPwPu57fP334Hfofubvbavgb6GG+neJ1/t9+mRi+zPy+i6ks5J8jO6Vs39G/Mf3M9zPfB14F1V9eV+2lOBrwzc+LDdSfmPi5aNJKcCG6vqVZOuRVsuyZPourGOnXQt+o0k5wInVtV3J13LqEzlh5Qk3V5/R9tin97WmFXV/FbedseuJ0lSk11PkqQmWxSSpKbt7hrFihUr6qCDDpp0GZJ0p3LeeeddWVUrh03b7oLioIMOYnZ2dtJlSNKdSpKLFppm15MkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTdvdB+6kcen+h83o+X1smrSJtiiSnJLkiiRDv8c9yeFJrk1yfv/z6nHXKC2kqrb4Z2uWkyZt0i2KU4F30P1/54WcVVV/NJ5yJEnzTbRFUVXr6P4lpyRpSt0ZLmYfluTbST6d5IHDZkiyKslsktlNmzaNuz5J2q5Ne1B8Cziwqh4CvB34xLCZqmp1Vc1U1czKlUO/JVeStJWmOiiq6mdVdX3/+ExgpyQrJlyWJC0rUx0USfZOfw9ikkPp6r1qslVJ0vIy0buekpwBHA6sSLIROBnYCaCq3gMcB/xZkpuBm4Djy/sFJWmsJhoUVXXCItPfQXf7rCRpQqa660mSNHkGhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktQ00aBIckqSK5J8d4HpSfL3SdYn+U6Sh427Rkla7ibdojgVOKox/Wjg4P5nFfDuMdQkSRow0aCoqnXA5sYsxwCnV+ccYI8k+4ynOkkSTL5FsZj9gIsHhjf24yRJYzLtQbEkSVYlmU0yu2nTpkmXI0nblWkPikuAew8M79+Pu42qWl1VM1U1s3LlyrEVJ0nLwbQHxRrgOf3dT48Erq2qyyZdlCQtJztOcuNJzgAOB1Yk2QicDOwEUFXvAc4EngKsB24Enj+ZSiVp+ZpoUFTVCYtML+DFYypHkjTEtHc9SZImzKCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVLTRIMiyVFJfpBkfZKXD5n+vCSbkpzf/7xwEnVK0nK246Q2nGQH4J3AkcBG4JtJ1lTV9+bN+tGqOmnsBUqSgMm2KA4F1lfVhqr6JfAR4JgJ1iNJGmKSQbEfcPHA8MZ+3HzPTPKdJB9Lcu/xlCZJmjPtF7P/DTioqh4MfB44bdhMSVYlmU0yu2nTprEWKEnbu0kGxSXAYAth/37crarqqqr6RT/4PuDhw1ZUVauraqaqZlauXDmSYiVpuZpkUHwTODjJfZLsDBwPrBmcIck+A4NPAy4cY32SJJZw11OSuwE3VdUtSX4X+D3g01X1qzuy4aq6OclJwGeBHYBTquqCJK8DZqtqDfAXSZ4G3AxsBp53R7YpSdpyqar2DMl5wGOAPYGv0rUEfllVzx59eVtuZmamZmdnJ12GNFQSFnvPSZOQ5Lyqmhk2bSldT6mqG4FnAO+qqmcBD9yWBUqSpteSgiLJYcCzgU/143YYXUmSpGmylKD4r8ArgI/31xDuC3x5pFVJkqbGohezq2otsDbJXfvhDcBfjLowSdJ0WLRFkeSwJN8Dvt8PPyTJu0ZemSRpKiyl6+l/AU8GrgKoqm8Djx1hTZKkKbKkD9xV1cXzRv16BLVIkqbQUr5m/OIk/x6oJDsBf4mfkJakZWMpLYo/BV5M982ulwAP7YclScvAUu56upLuMxSSpGVoKd/19AHgdt85UFUvGElFkqSpspRrFJ8ceLwL8HTg0tGUI0maNkvpevrnweEkZwBnj6wiaUL22msvrr766pFvJ8lI17/nnnuyefPmkW5Dy8tSWhTzHQzcc1sXIk3a1VdfvV18s+uog0jLz1KuUVxHd40i/e/LgZeNuC5J0pRYStfTbuMoRJI0nRYMiiQPay1YVd/a9uVIkqZNq0Xxt41pBTxhG9ciSZpCCwZFVT1+nIVIkqbTku56SvL7wCF0n6MAoKpOH1VRkqTpsZS7nk4GDqcLijOBo+k+R2FQSNIysJQvBTwOOAK4vKqeDzwE2H2kVUmSpsZSguLnVXULcHOSuwNXAPcebVmSpGnRuj32ncAZwDeS7AG8FzgPuB74+liqkyRNXOsaxf8F3gLsC9xAFxpHAnevqu+MoTZJ0hRYsOupqt5WVYfR/X/sq4BTgM8AT09y8JjqkyRN2KLXKKrqoqp6c1X9AXACcCzw/VEXJkmaDosGRZIdkzw1yYeATwM/AJ4x8sokSVOhdTH7SLoWxFOAbwAfAVZV1Q1jqk2SNAVaF7NfAXwYeGlVjf6/uUiSplLrYvYTqup9owyJJEcl+UGS9UlePmT6XZJ8tJ9+bpKDRlWLJGm4pXzgbiSS7AC8k+4rQQ4BTkhyyLzZTgSurqr7AW8F3jzeKiVJEwsK4FBgfVVtqKpf0l0DOWbePMcAp/WPPwYcEf/PoySN1SSDYj/g4oHhjf24ofNU1c3AtcA95q8oyaoks0lmN23aNKJyJWl5mmRQbDNVtbqqZqpqZuXKlZMuR5K2K5MMiku47ZcL7t+PGzpPkh3pvrX2qrFUJ0kCJhsU3wQOTnKfJDsDxwNr5s2zBnhu//g44EtVVWOsUZKWvSX9h7tRqKqbk5wEfBbYATilqi5I8jpgtqrWAO8HPphkPbCZLkwkSWM0saAAqKoz6f5r3uC4Vw88/jnwrHHXJUn6je3iYrYkaXQMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUtNEv8JDmiZ18t3hNbtPuow7rE6++6RL0HbGoJB6ee3P2B6+nDgJ9ZpJV6HtiV1PkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUNJGgSLJXks8n+WH/e88F5vt1kvP7nzXjrlOSNLkWxcuBL1bVwcAX++Fhbqqqh/Y/TxtfeZKkOZMKimOA0/rHpwHHTqgOSdIiJhUU96qqy/rHlwP3WmC+XZLMJjknybELrSzJqn6+2U2bNm3rWiVpWdtxVCtO8gVg7yGTXjk4UFWVpBZYzYFVdUmS+wJfSvJ/qupH82eqqtXAaoCZmZmF1iVJ2gojC4qqeuJC05L8NMk+VXVZkn2AKxZYxyX97w1JvgL8AXC7oJAkjc6kup7WAM/tHz8X+Nf5MyTZM8ld+scrgEcB3xtbhZIkYHJB8SbgyCQ/BJ7YD5NkJsn7+nkeAMwm+TbwZeBNVWVQSNKYjazrqaWqrgKOGDJ+Fnhh//hrwIPGXJokaZ6JBIU0rZJMuoQ7bM89h35+VdpqBoXUqxr9DXNJxrIdaVvyu54kSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkpokERZJnJbkgyS1JZhrzHZXkB0nWJ3n5OGuUJHUm1aL4LvAMYN1CMyTZAXgncDRwCHBCkkPGU54kac6Ok9hoVV0IkKQ126HA+qra0M/7EeAY4HsjL1CSdKuJBMUS7QdcPDC8EXjEsBmTrAJWARxwwAGjr0xi0ROdbbZcVW3VdqRtZWRBkeQLwN5DJr2yqv51W26rqlYDqwFmZmZ8V2ksPIBruRhZUFTVE+/gKi4B7j0wvH8/TpI0RtN8e+w3gYOT3CfJzsDxwJoJ1yRJy86kbo99epKNwGHAp5J8th+/b5IzAarqZuAk4LPAhcA/VtUFk6hXkpazSd319HHg40PGXwo8ZWD4TODMMZYmSZpnmrueJElTwKCQJDUZFJKkJoNCktSU7e1DQ0k2ARdNug5pASuAKyddhDTEgVW1ctiE7S4opGmWZLaqFvzGZGka2fUkSWoyKCRJTQaFNF6rJ12AtKW8RiFJarJFIUlqMigkSU0GhTQGSU5JckWS7066FmlLGRTSeJwKHDXpIqStYVBIY1BV64DNk65D2hoGhSSpyaCQJDUZFJKkJoNCktRkUEhjkOQM4OvA/ZNsTHLipGuSlsqv8JAkNdmikCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTf8f+xnaypm0MQcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming data_to_study_filled_drop is your DataFrame\n",
    "plt.boxplot(data_to_study_filled_drop['Growth +1'], showfliers=False)\n",
    "plt.title('Boxplot of Growth +5 (Without Outliers)')\n",
    "plt.ylabel('Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_to_study_filled_drop['target'] = data_to_study_filled_drop['Growth +5'] > 0\n",
    "# data_to_study_filled_drop['target'] = data_to_study_filled_drop['target'].astype(int)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a DataFrame named data_to_study_final\n",
    "# data_to_study_filled_drop['target'] = np.where(data_to_study_filled_drop['Growth +5'] < 0, 0, \n",
    "#                         np.where(data_to_study_filled_drop['Growth +5'] <= data_to_study_filled_drop['Growth +5'].quantile(0.9), 1, 2))\n",
    "\n",
    "data_to_study_filled_drop['target'] = np.where(data_to_study_filled_drop['Growth +1'] < 0, 0, 1)\n",
    "\n",
    "# data_to_study_filled_drop['target'] = np.where(data_to_study_filled_drop['Growth +5'] < 0, 0, \n",
    "#                                     np.where(data_to_study_filled_drop['Growth +5'] < 2.5, 1, 2))\n",
    "\n",
    "num_classes = len(data_to_study_filled_drop['target'].unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1483\n",
       "1     904\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_study_filled_drop['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859185476302782"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_study_filled_drop['Growth +1'].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_study_final = data_to_study_filled_drop.drop(columns='Growth +5')\n",
    "data_to_study_final = data_to_study_filled_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2387, 9)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_study_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1483\n",
       "1     904\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_study_final['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your financial data into a DataFrame (replace 'your_data.csv' with your dataset)\n",
    "df = data_to_study_final.copy()\n",
    "\n",
    "# Define the sequence length and other hyperparameters\n",
    "n_steps = 5 # years to study prior\n",
    "# n_features = len(df.columns) - 1  # Replace with the number of feature columns in your dataset\n",
    "num_companies = len(df['ticker'].unique())  # Assuming you have a 'ticker' column\n",
    "\n",
    "# Use LabelEncoder to convert 'ticker' values to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['ticker'] = label_encoder.fit_transform(df['ticker'])\n",
    "\n",
    "# Prepare an empty list to store sequences for all companies\n",
    "all_sequences = []\n",
    "all_targets = []  # To store the corresponding target values\n",
    "all_sequences_train = []\n",
    "all_sequences_test = []\n",
    "all_targets_train = []  # To store the corresponding target values\n",
    "all_targets_test = []  # To store the corresponding target values\n",
    "all_info_rows_test = []\n",
    "all_info_rows_train = []\n",
    "\n",
    "# Define the percentage of data to allocate for testing\n",
    "test_size = 0.2\n",
    "\n",
    "# Iterate through unique company labels\n",
    "for label in df['ticker'].unique():\n",
    "    # Filter data for the current company label\n",
    "    company_data = df[df['ticker'] == label]\n",
    "\n",
    "    # Sort data by date\n",
    "    company_data_complete = company_data.sort_values(by='Date')\n",
    "    company_data = company_data.sort_values(by='Date').drop(columns=['Date', 'ticker', 'Growth +1'])\n",
    "    n_features = len(company_data.columns) - 1 \n",
    "    # Create sequences for the current company\n",
    "    sequences = []\n",
    "    targets = []  # To store the corresponding target values\n",
    "    info_rows = []\n",
    "    sequences_train = []\n",
    "    sequences_test = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    for i in range(len(company_data) - n_steps + 1):\n",
    "        sequence = company_data.iloc[i:i + n_steps, 0:n_features].values  # Adjust column indices\n",
    "        target = company_data.iloc[i + n_steps - 1, n_features]  # Assuming the target column is the last one\n",
    "        info_row = company_data_complete.iloc[i + n_steps - 1]\n",
    "        sequences.append(sequence)\n",
    "        targets.append(target)\n",
    "        info_rows.append(info_row)\n",
    "\n",
    "    # Calculate the index to split the data\n",
    "    split_index = int(len(targets) * (1 - test_size))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    sequences_train = sequences[:split_index]\n",
    "    sequences_test = sequences[split_index:]\n",
    "    targets_train = targets[:split_index]\n",
    "    targets_test = targets[split_index:]\n",
    "    info_rows_train = info_rows[:split_index]\n",
    "    info_rows_test = info_rows[split_index:]\n",
    "\n",
    "    # Append sequences and corresponding targets to the lists\n",
    "    all_sequences.extend(sequences)\n",
    "    all_targets.extend(targets)\n",
    "    # Append sequences and corresponding targets to the lists\n",
    "    all_sequences_train.extend(sequences_train)\n",
    "    all_sequences_test.extend(sequences_test)\n",
    "    all_targets_train.extend(targets_train)\n",
    "    all_targets_test.extend(targets_test)\n",
    "    all_info_rows_train.extend(info_rows_train)\n",
    "    all_info_rows_test.extend(info_rows_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define the percentage of data to allocate for testing\n",
    "# test_size = 0.2\n",
    "\n",
    "# # Calculate the index to split the data\n",
    "# split_index = int(len(all_targets) * (1 - test_size))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = all_sequences_train, all_sequences_test\n",
    "y_train, y_test = all_targets_train, all_targets_test\n",
    "\n",
    "train_all_info, test_all_info = all_info_rows_train, all_info_rows_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define the LSTM model for classification\n",
    "class LSTMClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassificationModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step's output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = len(all_sequences[0][0])  # Number of features in each sequence\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "output_size = num_classes  # Three classes for classification (0, 1, 2)\n",
    "\n",
    "# Create the classification model\n",
    "classification_model = LSTMClassificationModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define the loss function for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved precision for class 1: 0.5333\n",
      "Epoch [10/400], Loss: 0.6560\n",
      "Epoch [20/400], Loss: 0.6324\n",
      "Epoch [30/400], Loss: 0.6033\n",
      "Epoch [40/400], Loss: 0.5656\n",
      "Epoch [50/400], Loss: 0.5212\n",
      "Epoch [60/400], Loss: 0.4738\n",
      "Epoch [70/400], Loss: 0.4251\n",
      "Epoch [80/400], Loss: 0.3772\n",
      "Epoch [90/400], Loss: 0.3316\n",
      "Epoch [100/400], Loss: 0.2909\n",
      "Epoch [110/400], Loss: 0.2593\n",
      "Epoch [120/400], Loss: 0.2246\n",
      "Epoch [130/400], Loss: 0.2024\n",
      "Epoch [140/400], Loss: 0.1746\n",
      "Epoch [150/400], Loss: 0.1531\n",
      "Epoch [160/400], Loss: 0.1385\n",
      "Epoch [170/400], Loss: 0.1245\n",
      "Epoch [180/400], Loss: 0.1093\n",
      "Epoch [190/400], Loss: 0.1041\n",
      "Epoch [200/400], Loss: 0.0913\n",
      "Epoch [210/400], Loss: 0.0826\n",
      "Epoch [220/400], Loss: 0.0867\n",
      "Epoch [230/400], Loss: 0.0730\n",
      "Epoch [240/400], Loss: 0.0635\n",
      "Epoch [250/400], Loss: 0.0592\n",
      "Epoch [260/400], Loss: 0.0738\n",
      "Epoch [270/400], Loss: 0.0562\n",
      "Epoch [280/400], Loss: 0.0500\n",
      "Epoch [290/400], Loss: 0.0458\n",
      "Epoch [300/400], Loss: 0.0424\n",
      "Epoch [310/400], Loss: 0.0392\n",
      "Epoch [320/400], Loss: 0.0364\n",
      "Epoch [330/400], Loss: 0.0367\n",
      "Epoch [340/400], Loss: 0.0782\n",
      "Epoch [350/400], Loss: 0.0583\n",
      "Epoch [360/400], Loss: 0.0382\n",
      "Epoch [370/400], Loss: 0.0355\n",
      "Epoch [380/400], Loss: 0.0304\n",
      "Epoch [390/400], Loss: 0.0279\n",
      "Epoch [400/400], Loss: 0.0259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classification model\n",
    "num_epochs = 400\n",
    "best_precision = 0.0  # Initialize the best precision for class 1\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = classification_model(X_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Evaluate precision on class 1 (target = 1)\n",
    "    classification_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = classification_model(X_test)\n",
    "\n",
    "        # Apply softmax to obtain class probabilities\n",
    "        class_probs = torch.nn.functional.softmax(test_outputs, dim=1)\n",
    "        _, predicted = torch.max(test_outputs, 1)  # Get the predicted class labels\n",
    "\n",
    "        # Convert the predictions and class probabilities to numpy arrays\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "        class_probs = class_probs.cpu().numpy()\n",
    "\n",
    "    # Calculate precision for class 1\n",
    "    precision_class_1 = precision_score(y_test, y_pred, labels=[1], average='macro')\n",
    "\n",
    "    # Check if precision for class 1 is better than the best seen so far\n",
    "    if precision_class_1 > best_precision:\n",
    "        best_precision = precision_class_1\n",
    "        best_model = classification_model.state_dict()\n",
    "        print(f'Improved precision for class 1: {best_precision:.4f}')\n",
    "\n",
    "# Use the best model with the highest precision for class 1\n",
    "classification_model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classification model on the test data\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = classification_model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)  # Get the predicted class labels\n",
    "\n",
    "# Convert the predictions and true labels to numpy arrays\n",
    "y_pred = predicted.cpu().numpy()\n",
    "y_true = y_test.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter class_probs for examples where y_pred is equal to 1\n",
    "indices_class_1 = (y_pred == 1)  # Get indices where y_pred is equal to 1\n",
    "class_probs_class_1 = class_probs[indices_class_1]  # Filter class_probs\n",
    "\n",
    "# Create y_pred_mod based on class_probs_class_1\n",
    "y_pred_mod = np.zeros_like(y_pred)\n",
    "y_pred_mod[indices_class_1] = (class_probs_class_1[:, 1] > 0.9).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False,  True,  True,  True,  True, False,\n",
       "       False, False,  True,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True,  True, False, False,  True,  True,  True,\n",
       "       False, False,  True,  True,  True, False,  True,  True, False,\n",
       "        True, False,  True, False, False,  True, False, False, False,\n",
       "        True,  True,  True, False, False,  True,  True,  True, False,\n",
       "        True, False, False, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        True,  True,  True, False, False,  True, False, False,  True,\n",
       "       False, False,  True, False,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True,  True, False,  True, False,\n",
       "        True, False, False,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(class_probs_class_1[:, 1] > 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_form(y_true, y_pred):    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-score: {f1:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5552\n",
      "Precision: 0.5395\n",
      "Recall: 0.5532\n",
      "F1-score: 0.5155\n",
      "Confusion Matrix:\n",
      "[[122  97]\n",
      " [ 32  39]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification metrics\n",
    "metrics_form(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6103\n",
      "Precision: 0.5078\n",
      "Recall: 0.5088\n",
      "F1-score: 0.5066\n",
      "Confusion Matrix:\n",
      "[[155  64]\n",
      " [ 49  22]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification metrics\n",
    "metrics_form(y_true, y_pred_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2867647058823529"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate precision for class 1\n",
    "precision_class_1 = precision_score(y_true, y_pred, labels=[1], average='macro')\n",
    "precision_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2558139534883721"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate precision for class 1\n",
    "precision_class_1 = precision_score(y_true, y_pred_mod, labels=[1], average='macro')\n",
    "precision_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5492957746478874"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate precision for class 1\n",
    "recall_class_1 = recall_score(y_true, y_pred, labels=[1], average='macro')\n",
    "recall_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30985915492957744"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate precision for class 1\n",
    "recall_class_1 = recall_score(y_true, y_pred_mod, labels=[1], average='macro')\n",
    "recall_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
